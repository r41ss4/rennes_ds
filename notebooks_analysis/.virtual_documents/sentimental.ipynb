


# Import libs
import pandas as pd
import numpy as np
import seaborn as sb
import matplotlib.pyplot as plt 
from textblob import TextBlob


# Read df of products
df_prod = pd.read_csv('https://raw.githubusercontent.com/r41ss4/rennes_ds/refs/heads/main/data/cleaned/clean_products.csv')
df_prod.head(5)


# Review columns
df_prod.columns


df_prod[['product_id', 'product_name', 'loves_count', 'rating', 'reviews']].head()


# Read df of products
df_rev = pd.read_csv('https://raw.githubusercontent.com/r41ss4/rennes_ds/refs/heads/main/data/cleaned/clean_reviews.csv')
df_rev.head(5)


# Eliminate column Unnamed: 0
df_rev.drop('Unnamed: 0', axis=1, inplace=True)
# Review changes
df_rev.columns


# Reviews general info 
df_rev.info()


# Rename columns for practicity
df_rev.rename(columns = {'review_text':'text'}, inplace = True)
df_rev.head()


# Subset data
df_rev_s = df_rev[['text','is_recommended', 'rating', 'product_id', 'product_name', 'brand_name']]
# Review 
df_rev_s.head()


df_rev_s['is_recommended'].value_counts()


# Calculate percentage of is_recommended True
label_pos = (df_rev_s['is_recommended'].value_counts()[1] / len(df_rev_s) *100).round(2)
label_pos


# Calculate percentage of is_recommended False
label_neg = (df_rev_s['is_recommended'].value_counts()[0] / len(df_rev_s) *100).round(2)
label_neg


# Print percentage of is_recommended 
print("Positive is_recommended percentage", label_pos, "%")
print("Negative is_recommended percentage", label_neg, "%")





# Import libs
import re
import nltk
from bs4 import BeautifulSoup
from nltk.corpus import stopwords
from nltk.tokenize import ToktokTokenizer
from nltk.stem import PorterStemmer


def preprocess_text(text, remove_digits=True):
    # Ensure the input is a string
    if not isinstance(text, str):
        text = str(text)
    
    # Removing HTML tags
    text = BeautifulSoup(text, "html.parser").get_text()
    
    # Removing square brackets
    text = re.sub(r'\[[^]]*\]', '', text)
    
    # Removing special characters
    if remove_digits:
        text = re.sub(r'[^a-zA-Z\s]', '', text)
    else:
        text = re.sub(r'[^a-zA-Z0-9\s]', '', text)
    
    # Lowercasing
    text = text.lower()
    
    # Stemming
    ps = PorterStemmer()
    text = ' '.join([ps.stem(word) for word in text.split()])
    
    # Removing stopwords
    stopword_list = set(stopwords.words('english'))
    tokenizer = ToktokTokenizer()
    tokens = tokenizer.tokenize(text)
    filtered_tokens = [token for token in tokens if token not in stopword_list]
    filtered_text = ' '.join(filtered_tokens)
    
    return filtered_text


# Print original review
print('Before preprocessing \n', df_rev['text'][3])


# Store review and clean it
df_rev['text'] = df_rev['text'].apply(preprocess_text)


print('After preprocessing \n', df_rev_s['text'][3])





# Define a function to get polarity
def get_polarity(text):
    analysis = TextBlob(str(text))
    return analysis.sentiment.polarity


# Apply the function to create a polarity column
df_rev['polarity'] = df_rev['text'].apply(get_polarity)

# Display the updated DataFrame with the new polarity column
df_rev.head()


# Get max of polarity
max_pol = df_rev['polarity'].max()
# Show result
max_pol


# Get min of polarity
min_pol = df_rev['polarity'].min()
# Show result
min_pol


# Define the bins and labels
bins = [-1.1,  -0.5,   0,   0.5,  1.1]
labels = [   1,    2,   3,     4]


# Create the 'polarity_level' column based on the bins and labels
df_rev['polarity_level'] = pd.cut(df_rev['polarity'], 
                                bins=bins, 
                                labels=labels)
df_rev.head()


# Change polarity_level as float
df_rev['polarity_level'] = df_rev['polarity_level'].astype('float')


# Review change
df_rev.info()





# Instal vaderSentiment if needed
!pip install vaderSentiment


# Import libs
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer


# Initialize VADER sentiment analyzer
analyzer = SentimentIntensityAnalyzer()


# Function to calculate sentiment intensity
def calculate_intensity(text):
    scores = analyzer.polarity_scores(text)
    return scores['compound']


# Apply intensity analysis
df_rev['intensity'] = df_rev['text'].apply(calculate_intensity)


# Review new column
df_rev.head(8)


# Get max of intensity
max_int = df_rev['intensity'].max()
# Show result
max_int


# Get min of intensity
min_int = df_rev['intensity'].min()
# Show result
min_int





# Import libs & download nltk content
import nltk
nltk.download('averaged_perceptron_tagger')
nltk.download('brown')
nltk.download('punkt')
nltk.download('wordnet')


# Function to extract aspects and their sentiments using TextBlob
def extract_aspects(text):
    blob = TextBlob(text)
    aspects = []
    for sentence in blob.sentences:
        for word, tag in sentence.tags:
            if tag in ['NN', 'NNS', 'NNP', 'NNPS']:
                aspect = word
                sentiment = sentence.sentiment.polarity
                aspects.append((aspect, sentiment))
    return aspects


# Apply aspect extraction
df_rev['aspects'] = df_rev['text'].apply(extract_aspects)


# Review new column
df_rev.head(8)





# Import libs 
from collections import Counter
from scipy.stats import mode


# Review dataset columns
df_rev.columns


# Review amount of products by unique value product_id
df_rev['product_id'].unique


# Select the specified columns
df_focus = df_rev[['product_id', 'product_name', 'brand_name', 'polarity', 'polarity_level', 'intensity', 'aspects']]
# Review df_focus
df_focus


# Review amount of products by number of unique value product_id
df_focus['product_id'].nunique()


# Review datatype before processing
df_focus.info()


# Define the mode function for aspects
def calculate_mode(series):
    aspect_list = [item for sublist in series for item in sublist]
    aspect_words = [aspect[0] for aspect in aspect_list]
    most_common_aspect = Counter(aspect_words).most_common(1)
    return most_common_aspect[0][0] if most_common_aspect else None


# Group by product_id and calculate the mean of polarity, polarity_level, and intensity
df_sent = df_focus.groupby('product_id').agg({
    'polarity': 'mean',
    'polarity_level': 'mean',
    'intensity': 'mean',
    'aspects': calculate_mode
}).reset_index()


# Review new df
df_sent.head(10)


# Get df info
df_sent.info()


# Review amount of products by unique value product_id in new df
df_sent['product_id'].nunique()


# Review amount of products by unique value product_id in original df
df_rev['product_id'].nunique()





# Review amount of products by unique value product_id in product
df_prod['product_id'].nunique()


# Merging the DataFrames on 'order_id' with an inner join to keep only matching rows
df_prodsent = pd.merge(df_prod, df_sent, 
                     on='product_id', how='inner')


# Review result
df_prodsent.head(10)


# Review info
df_prodsent.info()





















